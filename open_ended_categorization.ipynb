{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Categorizing Open-Ended Survey Responses with AI\n",
    "## Using Real Social Science Data\n",
    "\n",
    "This notebook demonstrates how to use AI to categorize and analyze open-ended survey responses from social science research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Food Choices Survey\n",
    "\n",
    "We'll use the **Food Choices** dataset from Kaggle:\n",
    "- **Source**: Survey of 126 college students at Mercyhurst University\n",
    "- **Topic**: Food preferences and eating habits\n",
    "- **Key Question**: \"Why do you eat comfort food?\"\n",
    "- **Download**: https://www.kaggle.com/datasets/borapajo/food-choices\n",
    "\n",
    "This dataset contains real open-ended responses about comfort food choices, perfect for demonstrating categorization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Install and import kagglehub for dataset download\n",
    "# !pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load API key\n",
    "import getpass\n",
    "\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    openai.api_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "else:\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    \n",
    "print(\"✅ API key loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load and Explore the Data\n",
    "\n",
    "First, download the dataset from Kaggle and load it. For this demo, we'll create sample data similar to the actual dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the actual Food Choices dataset from Kaggle\n",
    "path = kagglehub.dataset_download(\"borapajo/food-choices\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# List files in the dataset\n",
    "import os\n",
    "files = os.listdir(path)\n",
    "print(\"\\nFiles in dataset:\")\n",
    "for file in files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Load the main CSV file\n",
    "csv_file = os.path.join(path, 'food_coded.csv')\n",
    "food_data = pd.read_csv(csv_file)\n",
    "\n",
    "print(f\"\\nDataset shape: {food_data.shape}\")\n",
    "print(f\"Columns: {list(food_data.columns)}\")\n",
    "\n",
    "# Look for comfort food related columns\n",
    "comfort_cols = [col for col in food_data.columns if 'comfort' in col.lower()]\n",
    "print(f\"\\nComfort food related columns:\")\n",
    "for col in comfort_cols:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for the comfort_food_reasons_coded column specifically\n",
    "print(\"Searching for coded comfort food reasons...\\n\")\n",
    "\n",
    "# Get exact column names containing 'comfort'\n",
    "comfort_related = [col for col in food_data.columns if 'comfort' in col.lower()]\n",
    "print(f\"Columns with 'comfort' in name: {comfort_related}\")\n",
    "\n",
    "# Check if comfort_food_reasons_coded exists (with exact match)\n",
    "if 'comfort_food_reasons_coded' in food_data.columns:\n",
    "    print(\"\\n✅ Found 'comfort_food_reasons_coded' column!\")\n",
    "    coded_data = food_data['comfort_food_reasons_coded'].dropna()\n",
    "    print(f\"Non-null coded responses: {len(coded_data)}\")\n",
    "    print(f\"Unique categories: {coded_data.nunique()}\")\n",
    "    print(\"\\nCategory distribution:\")\n",
    "    print(coded_data.value_counts())\n",
    "    \n",
    "    # Check if we have both original and coded\n",
    "    if 'comfort_food_reasons' in food_data.columns:\n",
    "        # Create paired data\n",
    "        paired = food_data[['comfort_food_reasons', 'comfort_food_reasons_coded']].dropna()\n",
    "        print(f\"\\n✅ Found {len(paired)} responses with both text and codes!\")\n",
    "        print(\"\\nSample of paired data:\")\n",
    "        for idx, row in paired.head().iterrows():\n",
    "            print(f\"\\nText: {row['comfort_food_reasons']}\")\n",
    "            print(f\"Code: {row['comfort_food_reasons_coded']}\")\n",
    "else:\n",
    "    print(\"\\n❌ No 'comfort_food_reasons_coded' column found\")\n",
    "    print(\"We'll need to create our own categorization\")\n",
    "    \n",
    "    # Look for any text columns that might be comfort food reasons\n",
    "    text_cols = food_data.select_dtypes(include=['object']).columns\n",
    "    print(f\"\\nText columns that might contain reasons:\")\n",
    "    for col in text_cols:\n",
    "        if 'comfort' in col.lower() or 'reason' in col.lower():\n",
    "            print(f\"  - {col}\")\n",
    "            sample = food_data[col].dropna().head(2)\n",
    "            for val in sample:\n",
    "                print(f\"      Sample: {str(val)[:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: AI-Powered Categorization\n",
    "# If we have the actual comfort_food_reasons column, use it\n",
    "# Otherwise, create sample data\n",
    "\n",
    "if 'comfort_food_reasons' in food_data.columns:\n",
    "    # Use actual data\n",
    "    comfort_responses = food_data[['comfort_food_reasons']].dropna().copy()\n",
    "    comfort_responses['respondent_id'] = range(1, len(comfort_responses) + 1)\n",
    "    \n",
    "    # Add demographic data if available\n",
    "    if 'Gender' in food_data.columns:\n",
    "        comfort_responses['gender'] = food_data.loc[comfort_responses.index, 'Gender']\n",
    "    if 'grade_level' in food_data.columns:\n",
    "        comfort_responses['year'] = food_data.loc[comfort_responses.index, 'grade_level']\n",
    "    \n",
    "    print(f\"Using {len(comfort_responses)} actual responses from the dataset\")\n",
    "    print(\"\\nFirst 5 responses:\")\n",
    "    for idx, row in comfort_responses.head().iterrows():\n",
    "        print(f\"{row['respondent_id']}. {row['comfort_food_reasons']}\")\n",
    "    \n",
    "    # Check if we have existing codes for validation\n",
    "    if 'comfort_food_reasons_coded' in food_data.columns:\n",
    "        existing_codes = food_data.loc[comfort_responses.index, 'comfort_food_reasons_coded']\n",
    "        comfort_responses['existing_code'] = existing_codes\n",
    "        print(f\"\\n✅ Found existing codes for validation!\")\n",
    "        print(f\"Unique existing categories: {existing_codes.dropna().unique()}\")\n",
    "else:\n",
    "    # Create sample data if column not found\n",
    "    print(\"Creating sample data similar to typical comfort food responses...\")\n",
    "    comfort_responses = pd.DataFrame({\n",
    "        'respondent_id': range(1, 11),\n",
    "        'comfort_food_reasons': [\n",
    "            \"stress and anxiety from exams\",\n",
    "            \"reminds me of home\",\n",
    "            \"when I'm sad or lonely\",\n",
    "            \"boredom\",\n",
    "            \"tastes good\",\n",
    "            \"reward after hard work\",\n",
    "            \"nostalgia\",\n",
    "            \"homesickness\",\n",
    "            \"stress eating\",\n",
    "            \"convenience\"\n",
    "        ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the comfort food reasons column\n",
    "print(\"Examining comfort food reasons in the dataset:\\n\")\n",
    "\n",
    "# Check if there's a column with open-ended responses\n",
    "if 'comfort_food_reasons' in food_data.columns:\n",
    "    comfort_reasons = food_data['comfort_food_reasons'].dropna()\n",
    "    print(f\"Found {len(comfort_reasons)} comfort food reasons\")\n",
    "    print(\"\\nFirst 5 responses:\")\n",
    "    for i, reason in enumerate(comfort_reasons.head(), 1):\n",
    "        print(f\"{i}. {reason}\")\n",
    "        \n",
    "# Check for any coded/labeled versions\n",
    "if 'comfort_food_reasons_coded' in food_data.columns:\n",
    "    print(\"\\n✅ Found existing coded labels!\")\n",
    "    print(\"Unique codes:\", food_data['comfort_food_reasons_coded'].unique())\n",
    "    \n",
    "# Look for columns that might contain coding\n",
    "coded_cols = [col for col in food_data.columns if 'coded' in col.lower() or 'code' in col.lower()]\n",
    "if coded_cols:\n",
    "    print(f\"\\nColumns with potential coding:\")\n",
    "    for col in coded_cols:\n",
    "        print(f\"  - {col}: {food_data[col].dtype}\")\n",
    "        if food_data[col].dtype == 'object':\n",
    "            unique_vals = food_data[col].nunique()\n",
    "            print(f\"    Unique values: {unique_vals}\")\n",
    "            if unique_vals < 20:\n",
    "                print(f\"    Values: {food_data[col].unique()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def categorize_response(response, categories):\n",
    "    \"\"\"\n",
    "    Use AI to categorize an open-ended response.\n",
    "    \n",
    "    Args:\n",
    "        response: The open-ended text response\n",
    "        categories: Dictionary of category codes and descriptions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with primary and secondary categories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create category list for prompt\n",
    "    cat_list = \"\\n\".join([f\"- {code}: {desc}\" for code, desc in categories.items()])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Categorize this survey response about why someone eats comfort food.\n",
    "    \n",
    "    Response: \"{response}\"\n",
    "    \n",
    "    Categories:\n",
    "    {cat_list}\n",
    "    \n",
    "    Return a JSON object with:\n",
    "    - primary_category: The main category code\n",
    "    - secondary_category: A secondary category if applicable (or null)\n",
    "    - confidence: Your confidence level (0-1)\n",
    "    - key_phrases: List of 2-3 key phrases that led to this categorization\n",
    "    \n",
    "    Return ONLY the JSON object.\n",
    "    \"\"\"\n",
    "    \n",
    "    response_obj = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at qualitative data analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = response_obj['choices'][0]['message']['content'].strip()\n",
    "    if result.startswith(\"```json\"):\n",
    "        result = result[7:-3]\n",
    "    \n",
    "    return json.loads(result)\n",
    "\n",
    "# Test with one response\n",
    "test_response = comfort_responses.iloc[0]['comfort_food_reasons']\n",
    "test_result = categorize_response(test_response, coding_scheme)\n",
    "print(f\"Response: {test_response}\")\n",
    "print(f\"\\nCategorization: {json.dumps(test_result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Batch Processing All Responses\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Categorize all responses\n",
    "categorizations = []\n",
    "\n",
    "# Process only a subset if dataset is large\n",
    "max_responses = min(20, len(comfort_responses))  # Limit to 20 for demo\n",
    "print(f\"Processing {max_responses} responses...\")\n",
    "\n",
    "for idx in tqdm(range(max_responses), desc=\"Categorizing\"):\n",
    "    row = comfort_responses.iloc[idx]\n",
    "    try:\n",
    "        result = categorize_response(row['comfort_food_reasons'], coding_scheme)\n",
    "        result['respondent_id'] = row['respondent_id']\n",
    "        result['original_response'] = row['comfort_food_reasons']\n",
    "        \n",
    "        # Add existing code if available\n",
    "        if 'existing_code' in row:\n",
    "            result['existing_code'] = row['existing_code']\n",
    "            \n",
    "        categorizations.append(result)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing response {row['respondent_id']}: {e}\")\n",
    "        categorizations.append({\n",
    "            'respondent_id': row['respondent_id'],\n",
    "            'original_response': row['comfort_food_reasons'],\n",
    "            'primary_category': 'error',\n",
    "            'secondary_category': None,\n",
    "            'confidence': 0,\n",
    "            'key_phrases': []\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "results_df = pd.DataFrame(categorizations)\n",
    "print(f\"\\nProcessed {len(results_df)} responses\")\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: AI-Powered Categorization\n",
    "\n",
    "Now let's use AI to automatically categorize these responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_response(response, categories):\n",
    "    \"\"\"\n",
    "    Use AI to categorize an open-ended response.\n",
    "    \n",
    "    Args:\n",
    "        response: The open-ended text response\n",
    "        categories: Dictionary of category codes and descriptions\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with primary and secondary categories\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create category list for prompt\n",
    "    cat_list = \"\\n\".join([f\"- {code}: {desc}\" for code, desc in categories.items()])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Categorize this survey response about why someone eats comfort food.\n",
    "    \n",
    "    Response: \"{response}\"\n",
    "    \n",
    "    Categories:\n",
    "    {cat_list}\n",
    "    \n",
    "    Return a JSON object with:\n",
    "    - primary_category: The main category code\n",
    "    - secondary_category: A secondary category if applicable (or null)\n",
    "    - confidence: Your confidence level (0-1)\n",
    "    - key_phrases: List of 2-3 key phrases that led to this categorization\n",
    "    \n",
    "    Return ONLY the JSON object.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an expert at qualitative data analysis.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = response['choices'][0]['message']['content'].strip()\n",
    "    if result.startswith(\"```json\"):\n",
    "        result = result[7:-3]\n",
    "    \n",
    "    return json.loads(result)\n",
    "\n",
    "# Test with one response\n",
    "test_response = comfort_food_data.iloc[0]['comfort_food_reasons']\n",
    "test_result = categorize_response(test_response, coding_scheme)\n",
    "print(f\"Response: {test_response}\")\n",
    "print(f\"\\nCategorization: {json.dumps(test_result, indent=2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Batch Processing All Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Cross-tabulation Analysis\n",
    "# Only run if we have demographic data\n",
    "if 'gender' in comfort_responses.columns:\n",
    "    # Merge with demographic data\n",
    "    full_results = pd.merge(\n",
    "        results_df, \n",
    "        comfort_responses[['respondent_id', 'gender']], \n",
    "        on='respondent_id',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Cross-tabulation by gender\n",
    "    gender_crosstab = pd.crosstab(full_results['primary_category'], full_results['gender'])\n",
    "    print(\"Category distribution by gender:\")\n",
    "    print(gender_crosstab)\n",
    "    \n",
    "    # Visualization\n",
    "    gender_crosstab.plot(kind='bar', stacked=False)\n",
    "    plt.title('Comfort Food Reasons by Gender')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Gender')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No gender data available for cross-tabulation\")\n",
    "    full_results = results_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary category distribution\n",
    "primary_counts = results_df['primary_category'].value_counts()\n",
    "\n",
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Bar chart of primary categories\n",
    "primary_counts.plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('Primary Categories for Comfort Food Reasons')\n",
    "ax1.set_xlabel('Category')\n",
    "ax1.set_ylabel('Count')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Pie chart\n",
    "primary_counts.plot(kind='pie', ax=ax2, autopct='%1.1f%%')\n",
    "ax2.set_title('Distribution of Comfort Food Reasons')\n",
    "ax2.set_ylabel('')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nCategory Summary:\")\n",
    "for cat in primary_counts.index:\n",
    "    count = primary_counts[cat]\n",
    "    pct = (count / len(results_df)) * 100\n",
    "    print(f\"{cat}: {count} responses ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 10: Validation - Compare with Existing Coding (if available)\n",
    "if 'existing_code' in results_df.columns:\n",
    "    print(\"✅ Comparing AI categorization with existing human coding...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Filter to only rows with both AI and existing codes\n",
    "    validation_df = results_df[results_df['existing_code'].notna()].copy()\n",
    "    \n",
    "    if len(validation_df) > 0:\n",
    "        # Map existing codes to our categories if needed\n",
    "        # This is a simplified mapping - adjust based on actual codes\n",
    "        code_mapping = {\n",
    "            'stress': 'emotional_regulation',\n",
    "            'boredom': 'habit',\n",
    "            'sadness': 'emotional_regulation',\n",
    "            'happiness': 'reward',\n",
    "            'nostalgia': 'nostalgia',\n",
    "            'convenience': 'convenience',\n",
    "            'taste': 'sensory'\n",
    "        }\n",
    "        \n",
    "        # Apply mapping\n",
    "        validation_df['mapped_existing'] = validation_df['existing_code'].map(\n",
    "            lambda x: code_mapping.get(str(x).lower(), str(x).lower())\n",
    "        )\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        matches = (validation_df['primary_category'] == validation_df['mapped_existing']).sum()\n",
    "        total = len(validation_df)\n",
    "        accuracy = matches / total * 100\n",
    "        \n",
    "        print(f\"\\nAccuracy Score: {accuracy:.1f}% ({matches}/{total} matches)\")\n",
    "        \n",
    "        # Show mismatches for analysis\n",
    "        mismatches = validation_df[validation_df['primary_category'] != validation_df['mapped_existing']]\n",
    "        if len(mismatches) > 0:\n",
    "            print(f\"\\nMismatched categorizations ({len(mismatches)}):\")\n",
    "            for _, row in mismatches.head(5).iterrows():\n",
    "                print(f\"\\nResponse: {row['original_response'][:80]}...\")\n",
    "                print(f\"  Human code: {row['existing_code']}\")\n",
    "                print(f\"  AI category: {row['primary_category']}\")\n",
    "                print(f\"  AI confidence: {row['confidence']:.2f}\")\n",
    "        \n",
    "        # Confusion matrix\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        import numpy as np\n",
    "        \n",
    "        # Get unique categories\n",
    "        categories = sorted(set(validation_df['primary_category'].unique()) | \n",
    "                          set(validation_df['mapped_existing'].unique()))\n",
    "        \n",
    "        # Create confusion matrix\n",
    "        cm = confusion_matrix(validation_df['mapped_existing'], \n",
    "                            validation_df['primary_category'], \n",
    "                            labels=categories)\n",
    "        \n",
    "        # Plot confusion matrix\n",
    "        fig, ax = plt.subplots(figsize=(10, 8))\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        ax.figure.colorbar(im, ax=ax)\n",
    "        \n",
    "        ax.set(xticks=np.arange(cm.shape[1]),\n",
    "               yticks=np.arange(cm.shape[0]),\n",
    "               xticklabels=categories,\n",
    "               yticklabels=categories,\n",
    "               title='Confusion Matrix: Human vs AI Categorization',\n",
    "               ylabel='Human Coding',\n",
    "               xlabel='AI Categorization')\n",
    "        \n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm[i, j] > cm.max() / 2 else \"black\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Classification report\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(validation_df['mapped_existing'], \n",
    "                                   validation_df['primary_category']))\n",
    "    else:\n",
    "        print(\"No existing codes found for validation\")\n",
    "else:\n",
    "    print(\"No existing coding available for validation\")\n",
    "    print(\"Consider having a human expert code a subset for comparison\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze AI confidence levels\n",
    "confidence_scores = results_df['confidence'].values\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(confidence_scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(confidence_scores.mean(), color='red', linestyle='--', label=f'Mean: {confidence_scores.mean():.2f}')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('AI Confidence in Categorization')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Low confidence responses (for manual review)\n",
    "low_confidence = results_df[results_df['confidence'] < 0.7]\n",
    "print(f\"\\nResponses with low confidence (<0.7): {len(low_confidence)}\")\n",
    "if len(low_confidence) > 0:\n",
    "    print(\"\\nThese might need manual review:\")\n",
    "    for _, row in low_confidence.iterrows():\n",
    "        print(f\"- ID {row['respondent_id']}: {row['original_response'][:50]}...\")\n",
    "        print(f\"  Category: {row['primary_category']} (confidence: {row['confidence']:.2f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Cross-tabulation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge with demographic data\n",
    "full_results = pd.merge(results_df, comfort_food_data[['respondent_id', 'gender', 'year']], on='respondent_id')\n",
    "\n",
    "# Cross-tabulation by gender\n",
    "gender_crosstab = pd.crosstab(full_results['primary_category'], full_results['gender'])\n",
    "print(\"Category distribution by gender:\")\n",
    "print(gender_crosstab)\n",
    "\n",
    "# Visualization\n",
    "gender_crosstab.plot(kind='bar', stacked=False)\n",
    "plt.title('Comfort Food Reasons by Gender')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(title='Gender')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Extract Key Themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all key phrases\n",
    "all_phrases = []\n",
    "for phrases in results_df['key_phrases']:\n",
    "    if phrases:\n",
    "        all_phrases.extend(phrases)\n",
    "\n",
    "# Count phrase frequency\n",
    "phrase_counts = Counter(all_phrases)\n",
    "top_phrases = phrase_counts.most_common(10)\n",
    "\n",
    "print(\"Top 10 Key Phrases:\")\n",
    "for phrase, count in top_phrases:\n",
    "    print(f\"  '{phrase}': {count} occurrences\")\n",
    "\n",
    "# Create word cloud visualization (if wordcloud is installed)\n",
    "try:\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Create text from all phrases\n",
    "    text = ' '.join(all_phrases)\n",
    "    \n",
    "    # Generate word cloud\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Key Themes in Comfort Food Responses')\n",
    "    plt.show()\n",
    "except ImportError:\n",
    "    print(\"\\nInstall wordcloud for visualization: pip install wordcloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save categorized data\n",
    "output_file = 'categorized_comfort_food_responses.csv'\n",
    "full_results.to_csv(output_file, index=False)\n",
    "print(f\"Results saved to {output_file}\")\n",
    "\n",
    "# Create summary report\n",
    "summary = {\n",
    "    'total_responses': len(results_df),\n",
    "    'categories_used': list(primary_counts.index),\n",
    "    'most_common_category': primary_counts.index[0],\n",
    "    'average_confidence': confidence_scores.mean(),\n",
    "    'low_confidence_count': len(low_confidence),\n",
    "    'category_distribution': primary_counts.to_dict()\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY REPORT\")\n",
    "print(\"=\"*50)\n",
    "for key, value in summary.items():\n",
    "    if key != 'category_distribution':\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Save summary as JSON\n",
    "with open('categorization_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print(\"\\nSummary saved to categorization_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Validation - Compare with Manual Coding\n",
    "\n",
    "In a real research scenario, you would validate AI categorization against human coding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate manual coding for validation (in practice, this would be done by researchers)\n",
    "# Let's manually code a few responses for comparison\n",
    "\n",
    "manual_coding = {\n",
    "    1: 'emotional_regulation',  # \"stressed or anxious about exams\"\n",
    "    2: 'nostalgia',             # \"reminds me of home\"\n",
    "    3: 'emotional_regulation',  # \"sad or feeling lonely\"\n",
    "    4: 'habit',                 # \"Boredom\"\n",
    "    5: 'sensory'                # \"tastes good\"\n",
    "}\n",
    "\n",
    "# Compare AI vs manual for these samples\n",
    "print(\"Validation Comparison (AI vs Manual):\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for resp_id, manual_cat in manual_coding.items():\n",
    "    ai_result = results_df[results_df['respondent_id'] == resp_id].iloc[0]\n",
    "    match = \"✅\" if ai_result['primary_category'] == manual_cat else \"❌\"\n",
    "    \n",
    "    print(f\"\\nResponse {resp_id}: {match}\")\n",
    "    print(f\"  Text: {ai_result['original_response'][:60]}...\")\n",
    "    print(f\"  Manual: {manual_cat}\")\n",
    "    print(f\"  AI: {ai_result['primary_category']} (confidence: {ai_result['confidence']:.2f})\")\n",
    "\n",
    "# Calculate agreement rate\n",
    "matches = sum(1 for rid, manual in manual_coding.items() \n",
    "             if results_df[results_df['respondent_id'] == rid].iloc[0]['primary_category'] == manual)\n",
    "agreement_rate = matches / len(manual_coding) * 100\n",
    "print(f\"\\nAgreement Rate: {agreement_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions and Best Practices\n",
    "\n",
    "### Key Findings:\n",
    "1. **Efficiency**: AI can categorize hundreds of responses in minutes vs hours/days manually\n",
    "2. **Consistency**: AI applies the same criteria across all responses\n",
    "3. **Transparency**: Key phrases show why categories were assigned\n",
    "4. **Validation Needed**: Always validate with manual coding on a subset\n",
    "\n",
    "### Best Practices:\n",
    "1. **Clear Categories**: Define mutually exclusive, comprehensive categories\n",
    "2. **Iterative Refinement**: Review low-confidence responses and adjust prompts\n",
    "3. **Human Oversight**: AI assists but doesn't replace researcher judgment\n",
    "4. **Documentation**: Keep detailed records of prompts and decision rules\n",
    "5. **Validation**: Always validate on a subset with manual coding\n",
    "\n",
    "### When to Use AI Categorization:\n",
    "- Large datasets (100+ responses)\n",
    "- Initial exploration of themes\n",
    "- Consistent application of coding scheme\n",
    "- Time-sensitive analysis\n",
    "\n",
    "### Limitations:\n",
    "- May miss subtle context or sarcasm\n",
    "- Requires clear, well-defined categories\n",
    "- Costs scale with dataset size\n",
    "- Need for validation and quality checks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
