{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI-Powered Academic Research Workshop\n",
    "## Using OpenAI API for Literature Reviews and Paper Analysis\n",
    "\n",
    "This notebook provides practical examples of using AI tools for academic research workflows.\n",
    "\n",
    "### Learning goals\n",
    "- Understand what an API is and how to call it from Python\n",
    "- Learn to design and customize prompt templates for repeatable tasks\n",
    "- Use simple `for` loops to automate common research workflows\n",
    "- Structure outputs (JSON) so they are easy to store, filter, and analyze\n",
    "\n",
    "### What you’ll build\n",
    "- A single-paper analyzer that extracts key fields into JSON\n",
    "- A batch processor that loops over many papers with a progress bar\n",
    "- A relevance rater that scores papers against your research question\n",
    "- A short literature synthesis generated from multiple items\n",
    "\n",
    "### How to use this notebook\n",
    "- Run cells top-to-bottom the first time\n",
    "- Edit prompts in the `Prompt templates` section; re-run only the cells that depend on them\n",
    "- Use the `Loop patterns` section to demonstrate how iteration works step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New to APIs? Quick primer\n",
    "\n",
    "- **What is an API?** A way for your code to \"ask\" another service (like an AI model) to do something and return a result — like ordering from a menu and the kitchen delivers the dish.\n",
    "- **Why use it here?** We send a text prompt to an AI model and get a text response back.\n",
    "- **What do you need?**\n",
    "  - An account/key to prove who you are \n",
    "  - A small bit of setup to let Python talk to the service\n",
    "- **Costs/tokens:** Models bill per token (roughly word pieces). We'll show how to estimate and keep requests efficient.\n",
    "\n",
    "### How a request works (mental model)\n",
    "1) You prepare a prompt (your instructions)\n",
    "2) Your code sends it to the API endpoint\n",
    "3) The service runs the model and returns a response\n",
    "4) You parse the response and use it (print, save, analyze)\n",
    "\n",
    "```\n",
    "You  →  API endpoint  →  Model runs  →  Response back to you\n",
    "```\n",
    "\n",
    "### What “messages” mean here\n",
    "- We send both a short \"system\" message (role/context) and a \"user\" message (your prompt)\n",
    "- The model uses both to generate the reply\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-run checklist (you can copy-paste this into your notes)\n",
    "\n",
    "1) Create an account and an API key (your \"library card\").\n",
    "2) Store the key as an environment variable named `OPENAI_API_KEY`.\n",
    "   - macOS (temporary for current Terminal session):\n",
    "     - `export OPENAI_API_KEY=\"sk-...\"`\n",
    "   - macOS (persistent in `~/.zshrc`):\n",
    "     - Add this line: `export OPENAI_API_KEY=\"sk-...\"` and then run `source ~/.zshrc`\n",
    "3) Run the connection test cell to verify access.\n",
    "4) Run the examples; adjust `DEFAULT_MODEL` if desired.\n",
    "5) If batching many items, estimate tokens first to stay within budget.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Setting Up - Loading API Keys Securely\n",
    "\n",
    "First, we'll load the necessary libraries and set up secure API key management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# !pip install openai pandas requests PyPDF2 tqdm python-dotenv\n",
    "\n",
    "# If you encounter issues, try:\n",
    "# !pip install openai==0.28.1  # For older stable version\n",
    "# or\n",
    "# !pip install --upgrade openai  # For latest version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key set\n",
      "Ready to make API calls!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# For older OpenAI library (< 1.0)\n",
    "import openai\n",
    "\n",
    "# Get API key securely\n",
    "if os.getenv('OPENAI_API_KEY'):\n",
    "    openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "    print(\"✅ Using API key from environment\")\n",
    "else:\n",
    "    api_key = getpass.getpass(prompt=\"Enter your OpenAI API key: \")\n",
    "    openai.api_key = api_key\n",
    "    print(\"✅ API key set\")\n",
    "\n",
    "print(\"Ready to make API calls!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Basic API Query - Testing the Connection\n",
    "\n",
    "We’ll send a simple question and print the model’s reply.\n",
    "\n",
    "What to notice:\n",
    "- We pass a short `system` role for context (tone/role of the assistant)\n",
    "- The `temperature` controls randomness (0 = consistent; 1 = creative)\n",
    "- You can change `model` to a different one if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response from AI:\n",
      "A systematic literature review typically involves the following key components:\n",
      "\n",
      "1. Research question or objective: Clearly defining the research question or objective that the review aims to address is essential. This helps guide the search strategy and selection criteria for including studies.\n",
      "\n",
      "2. Search strategy: Developing a comprehensive search strategy to identify relevant studies is crucial. This involves identifying relevant databases, keywords, and search terms to ensure a thorough and systematic search.\n",
      "\n",
      "3. Study selection criteria: Establishing specific criteria for selecting studies to be included in the review helps ensure that the review is systematic and focused. These criteria may include factors such as publication date, study design, population characteristics, and outcome measures.\n",
      "\n",
      "4. Data extraction: Systematically extracting relevant data from the selected studies is a key component of a systematic literature review. This typically involves creating a data extraction form to record key information from each study, such as study design, sample size, findings, and conclusions.\n",
      "\n",
      "5. Quality assessment: Evaluating the quality of the included studies is important to assess the strength of the evidence and potential biases. Various tools and guidelines are available for assessing the quality of different types of studies.\n",
      "\n",
      "6. Data synthesis and analysis: Synthesizing the findings from the included studies involves analyzing and summarizing the data to identify patterns, trends, and gaps in the literature. Different methods of data synthesis, such as meta-analysis or narrative synthesis, may be used depending on the nature of the studies.\n",
      "\n",
      "7. Reporting: Clearly documenting the methods, findings, and conclusions of the systematic literature review is essential for transparency and reproducibility. Following established reporting guidelines, such as PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), can help ensure that the review is conducted and reported in a rigorous and transparent manner.\n"
     ]
    }
   ],
   "source": [
    "def simple_query(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    \"\"\"\n",
    "    Send a simple query to the OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        prompt: The question or prompt to send\n",
    "        model: The model to use (gpt-3.5-turbo is cost-effective)\n",
    "        temperature: Controls randomness (0=deterministic, 1=creative)\n",
    "    \n",
    "    Returns:\n",
    "        The model's response as a string\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful academic research assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=temperature\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Test the connection\n",
    "test_prompt = \"What are the key components of a systematic literature review?\"\n",
    "response = simple_query(test_prompt)\n",
    "print(\"Response from AI:\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Single Paper Review - Extracting Key Information\n",
    "\n",
    "We’ll extract a consistent JSON structure from a single paper.\n",
    "\n",
    "Why JSON?\n",
    "- Easy to parse in Python, save to CSV, or load into a database\n",
    "- Consistent fields make downstream analysis simpler (filter, aggregate, visualize)\n",
    "\n",
    "Prompt tips:\n",
    "- Say \"Return ONLY valid JSON\" to avoid extra text\n",
    "- Enumerate keys you want back (e.g., `research_question`, `methodology`, `key_findings`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis Result:\n",
      "{\n",
      "  \"research_question\": \"To what extent do AI tools impact academic research productivity?\",\n",
      "  \"methodology\": \"Surveys were conducted with 500 researchers to gather data on the use of AI tools in academic research.\",\n",
      "  \"key_findings\": [\n",
      "    \"AI tools reduce time spent on literature reviews by 40%.\",\n",
      "    \"AI tools reduce time spent on data analysis by 35%.\",\n",
      "    \"Concerns about accuracy and over-reliance on AI are key challenges.\"\n",
      "  ],\n",
      "  \"implications\": \"The findings suggest that AI tools can significantly improve research productivity but also raise concerns that need to be addressed.\",\n",
      "  \"limitations\": \"The study relies on self-reported data from researchers and may be subject to response bias.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "def analyze_single_paper(paper_title, paper_abstract):\n",
    "    \"\"\"\n",
    "    Analyze a single academic paper and extract key information.\n",
    "    \n",
    "    Args:\n",
    "        paper_title: The title of the paper\n",
    "        paper_abstract: The abstract of the paper\n",
    "    \n",
    "    Returns:\n",
    "        A dictionary with extracted information\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Please analyze this academic paper:\n",
    "    \n",
    "    Title: {paper_title}\n",
    "    \n",
    "    Abstract: {paper_abstract}\n",
    "    \n",
    "    Extract the following in JSON format:\n",
    "    - research_question: Main research question\n",
    "    - methodology: Brief description of methods\n",
    "    - key_findings: List of 3-5 main findings\n",
    "    - implications: Practical implications\n",
    "    - limitations: Any limitations mentioned\n",
    "    \n",
    "    Return ONLY the JSON object.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are an academic paper analyst. Return only JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0  # Use 0 for consistent extraction\n",
    "    )\n",
    "    \n",
    "    result_text = response['choices'][0]['message']['content'].strip()\n",
    "    \n",
    "    # Clean up JSON if needed\n",
    "    if result_text.startswith(\"```json\"):\n",
    "        result_text = result_text[7:-3]\n",
    "    \n",
    "    try:\n",
    "        return json.loads(result_text)\n",
    "    except:\n",
    "        return {\"error\": \"Failed to parse response\", \"raw\": result_text}\n",
    "\n",
    "# Example usage\n",
    "sample_title = \"The Impact of AI Tools on Academic Research Productivity\"\n",
    "sample_abstract = \"\"\"\n",
    "This study examines the impact of AI tools on academic research productivity. \n",
    "Through surveys (n=500) with researchers, we find that AI tools reduce time \n",
    "spent on literature reviews by 40% and data analysis by 35%. However, concerns \n",
    "about accuracy and over-reliance on AI emerge as key challenges.\n",
    "\"\"\"\n",
    "\n",
    "result = analyze_single_paper(sample_title, sample_abstract)\n",
    "print(\"Analysis Result:\")\n",
    "print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Batch Processing - Analyzing Multiple Papers\n",
    "\n",
    "We’ll loop over many items and collect results.\n",
    "\n",
    "What to notice:\n",
    "- A `for` loop iterates paper-by-paper\n",
    "- `tqdm` adds a progress bar so students see progress and timing\n",
    "- `try/except` ensures one failure doesn’t stop the whole batch\n",
    "\n",
    "Teaching tip:\n",
    "- Start with 2–3 items to keep it fast, then scale up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing batch of papers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing papers: 100%|██████████| 3/3 [00:05<00:00,  1.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Paper 1:\n",
      "{\n",
      "  \"research_question\": \"To examine machine learning applications in healthcare diagnosis and treatment\",\n",
      "  \"methodology\": \"Review of existing literature and case studies on machine learning in healthcare\",\n",
      "  \"key_findings\": [\n",
      "    \"Machine learning can improve diagnostic accuracy in healthcare\",\n",
      "    \"Machine learning can assist in personalized treatment plans for patients\",\n",
      "    \"Machine learning can help in predicting patient outcomes\"\n",
      "  ],\n",
      "  \"implications\": \"The findings suggest that integrating machine learning in healthcare can lead to more accurate diagnoses and personalized treatment plans, ultimately improving patient outcomes.\",\n",
      "  \"limitations\": \"One limitation mentioned is the need for large and diverse datasets for training machine learning models in healthcare.\"\n",
      "}\n",
      "\n",
      "Paper 2:\n",
      "{\n",
      "  \"research_question\": \"To study climate impacts on GDP\",\n",
      "  \"methodology\": \"Panel data analysis from 150 countries\",\n",
      "  \"key_findings\": [\n",
      "    \"Climate change has a negative impact on economic growth\",\n",
      "    \"Certain regions are more vulnerable to climate change effects\",\n",
      "    \"Mitigation strategies are crucial for sustaining economic growth\"\n",
      "  ],\n",
      "  \"implications\": \"Policymakers should prioritize climate change mitigation efforts to ensure long-term economic growth\",\n",
      "  \"limitations\": \"Limited to panel data analysis and may not capture all nuances of climate-economic interactions\"\n",
      "}\n",
      "\n",
      "Paper 3:\n",
      "{\n",
      "  \"research_question\": \"To what extent does the digital divide impact educational outcomes?\",\n",
      "  \"methodology\": \"The study utilized a mixed-methods approach, combining surveys and interviews to gather data on digital learning access and its effects on educational performance.\",\n",
      "  \"key_findings\": [\n",
      "    \"Significant disparities in digital learning access were found across different demographic groups.\",\n",
      "    \"Students from low-income families were particularly affected by the digital divide, leading to lower academic achievement.\",\n",
      "    \"Lack of access to digital resources hindered students' ability to participate in online learning activities.\"\n",
      "  ],\n",
      "  \"implications\": \"Policymakers and educators need to address the digital divide to ensure equitable access to education for all students.\",\n",
      "  \"limitations\": \"One limitation of the study was the reliance on self-reported data, which may introduce bias in the results.\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def process_paper_batch(papers_list):\n",
    "    \"\"\"\n",
    "    Process multiple papers in a loop.\n",
    "    \n",
    "    Args:\n",
    "        papers_list: List of dictionaries with 'title' and 'abstract'\n",
    "    \n",
    "    Returns:\n",
    "        List of analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # Process each paper with a progress bar\n",
    "    for paper in tqdm(papers_list, desc=\"Processing papers\"):\n",
    "        try:\n",
    "            result = analyze_single_paper(\n",
    "                paper_title=paper['title'],\n",
    "                paper_abstract=paper['abstract']\n",
    "            )\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append({\"error\": str(e)})\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Sample papers for demonstration\n",
    "sample_papers = [\n",
    "    {\n",
    "        'title': 'Machine Learning in Healthcare',\n",
    "        'abstract': 'This review examines ML applications in healthcare diagnosis and treatment.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Climate Change and Economic Growth',\n",
    "        'abstract': 'Using panel data from 150 countries, we study climate impacts on GDP.'\n",
    "    },\n",
    "    {\n",
    "        'title': 'Digital Divide in Education',\n",
    "        'abstract': 'COVID-19 exposed disparities in digital learning access across demographics.'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Process the batch\n",
    "print(\"Processing batch of papers...\")\n",
    "results = process_paper_batch(sample_papers)\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"\\nPaper {i+1}:\")\n",
    "    print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Advanced Literature Review with Relevance Scoring\n",
    "\n",
    "We’ll score each paper 1–5 for relevance to a research question and ask for a short explanation.\n",
    "\n",
    "Rubric idea:\n",
    "- 1–2: Tangential or off-topic\n",
    "- 3: Related but not a strong match\n",
    "- 4: Good match with several direct connections\n",
    "- 5: Strong, central match to the question\n",
    "\n",
    "Use cases:\n",
    "- Triage a large set quickly\n",
    "- Prioritize what to read first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the COVIDiSTRESS dataset\n",
    "# Dataset available at: https://osf.io/z39us/\n",
    "\n",
    "# For workshop, we'll simulate similar open-ended responses\n",
    "# In practice, download the actual CSV from the OSF repository\n",
    "\n",
    "# Create sample open-ended responses similar to COVIDiSTRESS survey\n",
    "sample_responses = [\n",
    "    {\n",
    "        'id': 1,\n",
    "        'response': \"Working from home has been challenging. I miss my colleagues and the office environment. The isolation is affecting my mental health.\",\n",
    "        'country': 'USA'\n",
    "    },\n",
    "    {\n",
    "        'id': 2,\n",
    "        'response': \"I'm worried about my elderly parents. Can't visit them due to restrictions. Financial stress from reduced work hours.\",\n",
    "        'country': 'UK'\n",
    "    },\n",
    "    {\n",
    "        'id': 3,\n",
    "        'response': \"Lost my job due to lockdown. Struggling to pay rent. But spending more time with family has been positive.\",\n",
    "        'country': 'India'\n",
    "    },\n",
    "    {\n",
    "        'id': 4,\n",
    "        'response': \"Healthcare system is overwhelmed. As a nurse, I'm exhausted and scared. Need more PPE and support.\",\n",
    "        'country': 'Italy'\n",
    "    },\n",
    "    {\n",
    "        'id': 5,\n",
    "        'response': \"Online learning is difficult for my children. Juggling work and homeschooling. Technology issues and lack of social interaction for kids.\",\n",
    "        'country': 'Canada'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "import pandas as pd\n",
    "responses_df = pd.DataFrame(sample_responses)\n",
    "print(f\"Loaded {len(responses_df)} open-ended responses\")\n",
    "responses_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7: Real-World Example - Categorizing Open-Ended Survey Responses\n",
    "\n",
    "Let's work with real survey data containing open-ended responses. We'll use the COVIDiSTRESS Global Survey dataset which includes written experiences from participants during the COVID-19 pandemic.\n",
    "\n",
    "### Dataset Information\n",
    "- **Source**: COVIDiSTRESS Global Survey (N=173,426)\n",
    "- **Period**: March 30 - May 30, 2020\n",
    "- **Contains**: Demographics, stress measures, and open-ended written experiences\n",
    "- **Download**: Available from Nature Scientific Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Research Question: \n",
      "How do AI tools like ChatGPT impact academic research productivity \n",
      "in literature reviews and data analysis?\n",
      "\n",
      "\n",
      "==================================================\n",
      "\n",
      "Paper: Machine Learning in Healthcare\n",
      "Relevance: 3/5\n",
      "Explanation: The paper is somewhat relevant as it discusses machine learning applications, but the focus is on healthcare rather than academic research productivity in literature reviews and data analysis.\n",
      "\n",
      "Paper: Climate Change and Economic Growth\n",
      "Relevance: 3/5\n",
      "Explanation: The paper's focus on climate impacts on GDP may not directly address the impact of AI tools on academic research productivity, but it provides relevant insights into the broader topic of climate change and economic growth.\n",
      "\n",
      "Paper: Digital Divide in Education\n",
      "Relevance: 3/5\n",
      "Explanation: The paper's focus on digital disparities in education is somewhat relevant to the research question about AI tools impacting academic research productivity.\n"
     ]
    }
   ],
   "source": [
    "def evaluate_paper_relevance(paper, research_question):\n",
    "    \"\"\"\n",
    "    Evaluate how relevant a paper is to your research question.\n",
    "    \n",
    "    Args:\n",
    "        paper: Dictionary with 'title' and 'abstract'\n",
    "        research_question: Your research question\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with relevance score and explanation\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Research Question: {research_question}\n",
    "    \n",
    "    Paper Title: {paper['title']}\n",
    "    Abstract: {paper['abstract']}\n",
    "    \n",
    "    Rate this paper's relevance (1-5) and explain why.\n",
    "    \n",
    "    Return JSON with:\n",
    "    - relevance_score: 1-5 (1=not relevant, 5=highly relevant)\n",
    "    - explanation: Brief explanation\n",
    "    - key_connections: List of 2-3 connections to research question\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are evaluating academic paper relevance. Return only JSON.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    result = response['choices'][0]['message']['content'].strip()\n",
    "    if result.startswith(\"```json\"):\n",
    "        result = result[7:-3]\n",
    "    \n",
    "    return json.loads(result)\n",
    "\n",
    "# Your research question\n",
    "my_research_question = \"\"\"\n",
    "How do AI tools like ChatGPT impact academic research productivity \n",
    "in literature reviews and data analysis?\n",
    "\"\"\"\n",
    "\n",
    "# Evaluate each sample paper\n",
    "print(f\"Research Question: {my_research_question}\\n\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for paper in sample_papers:\n",
    "    relevance = evaluate_paper_relevance(paper, my_research_question)\n",
    "    print(f\"\\nPaper: {paper['title']}\")\n",
    "    print(f\"Relevance: {relevance['relevance_score']}/5\")\n",
    "    print(f\"Explanation: {relevance['explanation']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Creating a Literature Review Summary\n",
    "\n",
    "We’ll create a concise synthesis across papers.\n",
    "\n",
    "Structure to aim for:\n",
    "- Common themes across the set\n",
    "- Key findings (evidence-based)\n",
    "- Gaps/limitations in current literature\n",
    "- Future research directions\n",
    "\n",
    "Tip:\n",
    "- Keep voice academic and precise; avoid claims not supported by the inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Literature Review Synthesis:\n",
      "==================================================\n",
      "Literature Review Synthesis\n",
      "\n",
      "1. COMMON THEMES:\n",
      "Across the papers on Machine Learning in Healthcare, Climate Change and Economic Growth, and Digital Divide in Education, common themes emerge regarding the utilization of AI tools like ChatGPT in enhancing academic research productivity. These themes include the integration of AI technologies to streamline literature reviews, improve data analysis processes, and facilitate knowledge synthesis in diverse academic fields.\n",
      "\n",
      "2. KEY FINDINGS:\n",
      "- Machine Learning in Healthcare: The application of machine learning algorithms in healthcare has shown promising results in predictive analytics, disease diagnosis, and personalized treatment recommendations.\n",
      "- Climate Change and Economic Growth: Studies have highlighted the interconnectedness between climate change and economic growth, emphasizing the need for data-driven approaches to address environmental challenges and foster sustainable development.\n",
      "- Digital Divide in Education: Research on the digital divide in education underscores the importance of leveraging AI tools to bridge technological disparities, enhance digital literacy, and promote equitable access to educational resources.\n",
      "\n",
      "3. GAPS:\n",
      "Despite the advancements in AI technologies like ChatGPT, there are notable gaps in current research concerning their specific impact on academic research productivity in literature reviews and data analysis. Limited studies have explored the effectiveness of AI tools in facilitating interdisciplinary research collaboration, addressing research bias, and ensuring data privacy and security in academic settings.\n",
      "\n",
      "4. FUTURE RESEARCH:\n",
      "Future research endeavors should focus on:\n",
      "- Conducting empirical studies to evaluate the efficiency and accuracy of AI tools like ChatGPT in conducting literature reviews and synthesizing research findings across various disciplines.\n",
      "- Investigating the ethical implications of AI utilization in academic research, including issues related to bias, transparency, and accountability.\n",
      "- Exploring innovative applications of AI technologies to enhance data visualization, knowledge dissemination, and research reproducibility in academic contexts.\n",
      "\n",
      "In conclusion, while AI tools like ChatGPT hold significant potential to revolutionize academic research productivity, further research is needed to address existing gaps and guide future advancements in this evolving field.\n"
     ]
    }
   ],
   "source": [
    "def generate_literature_synthesis(papers_analyses, research_question):\n",
    "    \"\"\"\n",
    "    Generate a synthesis of multiple papers for a literature review.\n",
    "    \n",
    "    Args:\n",
    "        papers_analyses: List of analyzed papers\n",
    "        research_question: Your research question\n",
    "    \n",
    "    Returns:\n",
    "        A structured literature review synthesis\n",
    "    \"\"\"\n",
    "    \n",
    "    # Format papers for the prompt\n",
    "    papers_text = \"\\n\\n\".join([\n",
    "        f\"Paper {i+1}: {paper.get('title', 'Unknown')}\\n\"\n",
    "        f\"Findings: {paper.get('key_findings', paper.get('abstract', 'N/A'))}\"\n",
    "        for i, paper in enumerate(papers_analyses)\n",
    "    ])\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    Research Question: {research_question}\n",
    "    \n",
    "    Papers:\n",
    "    {papers_text}\n",
    "    \n",
    "    Create a literature review synthesis with:\n",
    "    \n",
    "    1. COMMON THEMES: What themes appear across papers?\n",
    "    2. KEY FINDINGS: Main discoveries from these papers\n",
    "    3. GAPS: What's missing from current research?\n",
    "    4. FUTURE RESEARCH: What should be studied next?\n",
    "    \n",
    "    Keep it concise and academic.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are writing an academic literature review.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.3\n",
    "    )\n",
    "    \n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Generate synthesis using our sample papers and previous analyses\n",
    "print(\"Literature Review Synthesis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Combine our sample papers with any analysis results\n",
    "papers_for_synthesis = []\n",
    "for i, paper in enumerate(sample_papers):\n",
    "    papers_for_synthesis.append({\n",
    "        'title': paper['title'],\n",
    "        'abstract': paper['abstract'],\n",
    "        'key_findings': f\"Sample findings for {paper['title']}\"\n",
    "    })\n",
    "\n",
    "synthesis = generate_literature_synthesis(papers_for_synthesis, my_research_question)\n",
    "print(synthesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Tips\n",
    "\n",
    "### 1. API Key Management\n",
    "- Never hardcode API keys in your code\n",
    "- Use environment variables or .env files\n",
    "- Limit who has access; rotate keys if shared\n",
    "\n",
    "### 2. Cost & Throughput\n",
    "- Choose a cost-effective default model where possible\n",
    "- Count tokens for long prompts or big batches\n",
    "- Batch multiple items in one request when appropriate (vectorized prompt)\n",
    "\n",
    "### 3. Error Handling\n",
    "- Wrap API calls in try/except; keep going on failures\n",
    "- Handle common HTTP codes: 401 (auth), 429 (rate limit), 500 (server)\n",
    "- Consider short exponential backoff on retryable errors\n",
    "\n",
    "### 4. Data Quality\n",
    "- Validate outputs before using them (especially JSON)\n",
    "- Keep a small labeled set to sanity-check results\n",
    "- Maintain human oversight for critical judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Token counting for cost estimation\n",
    "import tiktoken\n",
    "\n",
    "def count_tokens(text, model=\"gpt-4\"):\n",
    "    \"\"\"Count tokens in a text string for cost estimation.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Example text\n",
    "example_text = \"This is a sample text to count tokens.\"\n",
    "token_count = count_tokens(example_text)\n",
    "print(f\"Token count: {token_count}\")\n",
    "print(f\"Estimated cost (GPT-4): ${token_count * 0.00003:.4f}\")  # Adjust pricing as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Exercises\n",
    "\n",
    "### Exercise 1: Customize the Analysis\n",
    "Modify the `analyze_single_paper` function to extract additional information relevant to your field.\n",
    "\n",
    "### Exercise 2: Build Your Dataset\n",
    "Create a CSV file with papers from your research area and process them using the batch processing function.\n",
    "\n",
    "### Exercise 3: Comparative Analysis\n",
    "Write a function that compares two papers and identifies similarities and differences.\n",
    "\n",
    "### Exercise 4: Citation Network\n",
    "Extract citation information and build a network of related papers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt templates: edit these in one place\n",
    "\n",
    "Centralize all prompts here so you can tweak examples without touching the loop or API logic.\n",
    "\n",
    "- Change wording, tone, and output format in one spot\n",
    "- Use placeholders like `{paper_title}`, `{paper_abstract}`, `{research_question}`\n",
    "- Keep output formats rigid when you need machine-readability (e.g., JSON)\n",
    "- When adding new templates, prefer triple-quoted strings and keep instructions concise\n",
    "\n",
    "Tip: Print a rendered prompt before sending to the API to sanity-check placeholders and formatting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict\n",
    "\n",
    "@dataclass\n",
    "class PromptTemplates:\n",
    "    # Single paper analysis in JSON\n",
    "    analyze_paper: str = (\n",
    "        \"\"\"\n",
    "        You are an academic paper analyst. Return ONLY valid JSON.\n",
    "        \n",
    "        Analyze this paper and extract:\n",
    "        - research_question\n",
    "        - methodology\n",
    "        - key_findings (3-5 bullet points)\n",
    "        - implications\n",
    "        - limitations\n",
    "        \n",
    "        Paper Title: {paper_title}\n",
    "        Abstract: {paper_abstract}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Relevance scoring with short explanation\n",
    "    relevance_scoring: str = (\n",
    "        \"\"\"\n",
    "        Return ONLY valid JSON with fields:\n",
    "        - relevance_score (1-5)\n",
    "        - explanation (<= 3 sentences)\n",
    "        - key_connections (2-3 bullets)\n",
    "        \n",
    "        Research Question: {research_question}\n",
    "        Paper Title: {paper_title}\n",
    "        Abstract: {paper_abstract}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "    # Synthesis prompt\n",
    "    synthesis: str = (\n",
    "        \"\"\"\n",
    "        You are writing a concise academic literature synthesis.\n",
    "        Given the list of papers, produce:\n",
    "        1) COMMON THEMES\n",
    "        2) KEY FINDINGS\n",
    "        3) GAPS\n",
    "        4) FUTURE RESEARCH\n",
    "        \n",
    "        Research Question: {research_question}\n",
    "        Papers (title and findings/abstracts):\n",
    "        {papers_text}\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "TEMPLATES = PromptTemplates()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop patterns: minimal, step-by-step, and vectorized\n",
    "\n",
    "These examples separate the loop logic from prompts and API calls. They print intuitive, teaching-friendly outputs:\n",
    "\n",
    "- What is being iterated\n",
    "- Which prompt is being used\n",
    "- What the model returned (pretty-printed)\n",
    "- How to switch models\n",
    "\n",
    "What each function demonstrates:\n",
    "- `loop_minimal`: the smallest working `for` loop + prompt\n",
    "- `loop_step_by_step`: prints each step, the prompt preview, and the result\n",
    "- `loop_vectorized_single_prompt`: combines multiple items into a single request for efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No-API mode: mock model for teaching\n",
    "\n",
    "Use the mock to teach loops and prompt design without any external calls.\n",
    "- Toggle `DRY_RUN = True`\n",
    "- The mock will show a prompt preview and a fake response header\n",
    "- Swap to real calls later by flipping `DRY_RUN = False`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo: zero-API exercises\n",
    "\n",
    "# 1) Minimal loop with mock output\n",
    "loop_minimal(sample_papers)\n",
    "\n",
    "# 2) Step-by-step loop with mock output\n",
    "loop_step_by_step(sample_papers, research_question=my_research_question)\n",
    "\n",
    "# 3) Vectorized single prompt with mock output\n",
    "loop_vectorized_single_prompt(sample_papers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary (1-minute read)\n",
    "\n",
    "- **API**: A way for programs to talk to services\n",
    "- **API key**: Your ID card for the service\n",
    "- **Prompt**: The instruction or question you send to the model\n",
    "- **Tokens**: Chunks of text used for billing/limits\n",
    "- **Temperature**: Controls randomness (lower = more consistent)\n",
    "- **Rate limit**: The maximum requests allowed in a time window (handle 429 errors with short delays)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting and privacy notes\n",
    "\n",
    "- 401 Unauthorized: check `OPENAI_API_KEY` is set correctly\n",
    "- 429 Rate limit: wait briefly and retry; reduce request rate or batch items\n",
    "- 500 Server error: retry after a short delay; if persistent, try later\n",
    "- Validate outputs: treat AI responses as drafts to review, not truth\n",
    "- Avoid sending sensitive data; anonymize where possible\n",
    "- For JSON outputs, keep prompts strict and add fallback parsing if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
